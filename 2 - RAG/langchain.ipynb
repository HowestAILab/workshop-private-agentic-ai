{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8166b5",
   "metadata": {},
   "source": [
    "# **Retrieval Augmented Generation**\n",
    "\n",
    "_Taalmodellen zijn heel krachtig, maar zijn qua kennis gelimiteerd tot hun trainingdata en kunnen halucineren. De antwoorden zijn niet gestaafd door een bronvermelding. Vragen over onze eigen documenten kan deze ook niet zomaar beantwoorden. Dit is waar Retrieval Augmented Generation (RAG) een oplossing biedt. Het is een techniek waarbij we onze eigen documenten kunnen bevragen met natuurlijke taal._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7608f5f",
   "metadata": {},
   "source": [
    "<img src=\"../.github/rag.png\" alt=\"RAG\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b11119",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80608e66",
   "metadata": {},
   "source": [
    "## **Voorbereiding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42890a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_milvus langchain-ollama ollama langchain_community langchain_text_splitters langchain_core pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397089f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d81c6",
   "metadata": {},
   "source": [
    "## **Embedder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf231f",
   "metadata": {},
   "source": [
    "<img src=\"../.github/tekst-embedding.png\" alt=\"Tekst embedding\" width=\"500\"/><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8b58d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedder = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97f69c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e45135",
   "metadata": {},
   "source": [
    "## **Vector database**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fee946",
   "metadata": {},
   "source": [
    "Zoals verteld tijdens de theorie, hebben we een **vector database** nodig om onze documenten in op te slaan.\n",
    "\n",
    "| Opslagmethode     | Gebruik                   |\n",
    "|-------------------|---------------------------|\n",
    "| ‚òÅÔ∏è Cloud             | Productie                 |\n",
    "| üêã Docker container  | Productie (eigen server)  |\n",
    "| üìÅ Lokaal bestand    | Development               |\n",
    "| üïì In-memory         | Development               |\n",
    "\n",
    "Wij gaan **Milvus** (open-source) gebruiken als lokaal bestand.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781edb7d",
   "metadata": {},
   "source": [
    "### **1. Aanmaken van database**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46db6f1",
   "metadata": {},
   "source": [
    "Zoals verteld in de theorie, moet je **documenten eerst embedden** alvorens ze toe te voegen aan de database.\n",
    "\n",
    "<img src=\"../.github/tekst-vectors-database.png\" alt=\"Tekst embedding\" width=\"500\"/><br>\n",
    "\n",
    "LangChain staat ons toe om de **embedder al mee te geven** bij het aanmaken van de database client. <br>\n",
    "We kunnen hierdoor later **rechtstreeks documenten toevoegen** zonder ze eerst manueel te embedden, LangChain zal dit automatisch doen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "URI = \"../milvus.db\"\n",
    "vectordb = Milvus(embedding_function=embedder, connection_args={\"uri\": URI})\n",
    "#                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9fdec3",
   "metadata": {},
   "source": [
    "### **2. Uitlezen van PDF**\n",
    "\n",
    "<img src=\"../.github/tekst-extractie.png\" alt=\"Tekst extractie\" width=\"400\"/>\n",
    "\n",
    "LangChain heeft een heleboel **voorgemaakte \"data loaders\"** om tekst uit bestanden te kunnen halen. <br>\n",
    "Zo is er eentje die gespecialiseerd is in het uitlezen van PDF bestanden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23cd66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Blogpost.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26f47a",
   "metadata": {},
   "source": [
    "### **3. Opsplitsen in kleinere sub-documenten**\n",
    "We kunnen in theorie de hele PDF omzetten naar 1 vector, maar daar zou heel **weinig informatie** in vervat zitten. <br>\n",
    "Om gerichter te kunnen zoeken, worden de documenten **opgesplitst in kleinere stukken** of \"chunks\". <br>\n",
    "LangChain beschikt opnieuw over **voorgemaakte \"text splitters\"** die dit voor ons kunnen doen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5354c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configureer een text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "\n",
    "# Splits de documenten in chunks\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3a546",
   "metadata": {},
   "source": [
    "### **4. Documenten in database stoppen**\n",
    "\n",
    "<img src=\"../.github/documenten-in-database-stoppen.png\" alt=\"Documenten in database stoppen\" width=\"500\"/><br>\n",
    "\n",
    "*Omdat we de embedder al hebben meegegeven bij het aanmaken van de database client, kunnen we rechtstreeks de documenten toevoegen.* <br>\n",
    "*LangChain zal ze automatisch embedden voor ons.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.add_documents(chunks)\n",
    "\n",
    "# Alternatieve manier met manuele embedding tussenstap:\n",
    "#\n",
    "# texts = [chunk.page_content for chunk in chunks]\n",
    "# vectors = embedder.embed_documents(chunks)\n",
    "# vectordb.add_embeddings(texts, vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fd240",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb7a9c",
   "metadata": {},
   "source": [
    "## **Database bevragen** *(= Retrieval)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1d516",
   "metadata": {},
   "source": [
    "### **5.1 Database bevragen**\n",
    "\n",
    "<img src=\"../.github/relevante-docs-zoeken.png\" alt=\"Documenten in database stoppen\" width=\"500\"/><br>\n",
    "\n",
    "*Omdat we de embedder al hebben meegegeven bij het aanmaken van de database client, kunnen we rechtstreeks de vraag in tekst-vorm meegeven.* <br>\n",
    "*LangChain zal deze automatisch embedden alvorens de zoekopdracht uit te voeren.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8513a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Waarover gaat de blogpost?\"\n",
    "\n",
    "# Zoeken in de database naar gerelateerde documenten\n",
    "documents_found = vectordb.search(question, search_type=\"similarity\", k=3)\n",
    "\n",
    "\n",
    "# Alternatieve manier met manuele embedding tussenstap:\n",
    "#\n",
    "# vector = embedder.embed_query(question)\n",
    "# documents = vectordb.similarity_search_by_vector(vector, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005019e2",
   "metadata": {},
   "source": [
    "### **5.2 Database bevragen** *(alternatieve manier)*\n",
    "\n",
    "Een **retriever** is een soort **LangChain component** die conceptueel ruimer is dan enkel zoeken in vector databases. <br>\n",
    "Ze kunnen ook andere bronnen aanspreken zoals het internet (als je dat zou willen). <br>\n",
    "Dit is meer best-practice in LangChain. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Waarover gaat de blogpost?\"\n",
    "\n",
    "# Maak een LangChain retriever object\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# Bevraag de database met de retriever\n",
    "documents_found = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4611d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b8540",
   "metadata": {},
   "source": [
    "## **Antwoord forumuleren** *(= Generation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b47d8c",
   "metadata": {},
   "source": [
    "### **6. Vraag & documenten aan taalmodel geven**\n",
    "\n",
    "<img src=\"../.github/antwoord-formuleren.png\" alt=\"Tekst extractie\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553dc863",
   "metadata": {},
   "source": [
    "LangChain standaardiseerd het proces om taalmodellen in te laden en aan te roepen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a977908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9eb4c",
   "metadata": {},
   "source": [
    "Er zijn ook **voorgemaakte prompts** beschikbaar die je kan importeren en **geoptimaliseerd zijn voor RAG**. <br>\n",
    "Wij gaan zelf eentje opstellen, voel je vrij om deze aan te passen om bv. een andere schrijfstijl te gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6af579c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_template = \"\"\"\n",
    "Je bent een assistent die vragen beantwoordt.\n",
    "Gebruik de volgende stukjes context om de vraag te beantwoorden.\n",
    "Vermeld altijd de bestandsnamen als bron bij je antwoorden.\n",
    "Als je het antwoord niet letterlijk in de context staat, zeg dan dat je het niet weet.\n",
    "Gebruik maximaal drie zinnen en houd het antwoord beknopt.\n",
    "\n",
    "Vraag: {question}\n",
    "\n",
    "Context:\\n{context}\n",
    "\n",
    "Antwoord: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We voegen de gevonden documenten samen tot 1 tekst en vermelden de bijhorende bestandsnamen\n",
    "context = \"\"\n",
    "for document in documents_found:\n",
    "    filename = document.metadata[\"source\"]\n",
    "    page = document.metadata[\"page_label\"]\n",
    "    text = document.page_content.strip()\n",
    "    context += f\"[{filename} - pagina {page}]\\n{text}\\n\\n\"\n",
    "\n",
    "# We injecteren de vraag en context in het prompt sjabloon\n",
    "prompt = prompt_template.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "# We geven de prompt aan de LLM\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551e530",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed149af",
   "metadata": {},
   "source": [
    "### **‚ùì Vraag:** Zou je een OpenAI embedder kunnen combineren met een LLM dat niet van OpenAI is (in een RAG context)?\n",
    "\n",
    "<details>\n",
    "  <summary>Antwoord onthullen (klikken)</summary>\n",
    "  \n",
    "  **‚úÖ Ja, dat kan!** <br>\n",
    "\n",
    "  Het embedden wordt enkel en alleen maar toegepast om **semantisch te kunnen zoeken** in de database naar documenten die betrekking hebben tot de vraag. <br>\n",
    "  We bewaren in de database **niet alleen de vector**, maar ook de bijhorende **oorsponkelijke chunk/tekst**. <br>\n",
    "  Het is die **zuivere tekst** die we gebruiken in een **prompt voor het taalmodel**. <br>\n",
    "  Het gebruikte embedding model staat dus volledig los van het taalmodel. <br>\n",
    "\n",
    "  Je kan perfect mixen en matchen met verschillende databases, taalmodellen en embedders. <br>\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

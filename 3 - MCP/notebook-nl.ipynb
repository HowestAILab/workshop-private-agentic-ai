{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0959318",
   "metadata": {},
   "source": [
    "# **Model Context Protocol**\n",
    "\n",
    "_Het Model Context Protocol of MCP - uitgevonden door Anthropic - is een gestandaardiseerde manier om taalmodellen toegang te geven tot bepaalde tools om acties uit te voeren. Nadat OpenAI dit open-source framework ook is beginnen gebruiken, werd het universeel geadopteerd als het standaard communicatieprotocol voor agents en LLM's. Vaak wordt MCP beschreven als de \"USB-C poort van taalmodellen\"._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d60ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1c5c4",
   "metadata": {},
   "source": [
    "## **Voorbereiding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6526d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/current/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=43798) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastmcp in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (2.10.2)\n",
      "Requirement already satisfied: langchain_mcp_adapters in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (0.1.8)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.5.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: authlib>=1.5.2 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from fastmcp) (1.6.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.2.2 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from fastmcp) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from fastmcp) (0.28.1)\n",
      "Requirement already satisfied: mcp>=1.10.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from fastmcp) (1.10.1)\n",
      "Requirement already satisfied: openapi-pydantic>=0.5.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from fastmcp) (0.5.1)\n",
      "Requirement already satisfied: pydantic>=2.11.7 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from pydantic[email]>=2.11.7->fastmcp) (2.11.7)\n",
      "Requirement already satisfied: python-dotenv>=1.1.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from fastmcp) (1.1.1)\n",
      "Requirement already satisfied: rich>=13.9.4 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from fastmcp) (14.0.0)\n",
      "Requirement already satisfied: typer>=0.15.2 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from fastmcp) (0.16.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.36 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langchain_mcp_adapters) (0.3.68)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langchain_mcp_adapters) (4.14.1)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: cryptography in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from authlib>=1.5.2->fastmcp) (45.0.5)\n",
      "Requirement already satisfied: anyio in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp) (4.9.0)\n",
      "Requirement already satisfied: certifi in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp) (1.0.9)\n",
      "Requirement already satisfied: idna in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.28.1->fastmcp) (0.16.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (0.4.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (24.2)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from mcp>=1.10.0->fastmcp) (0.4.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from mcp>=1.10.0->fastmcp) (4.24.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from mcp>=1.10.0->fastmcp) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from mcp>=1.10.0->fastmcp) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from mcp>=1.10.0->fastmcp) (2.4.1)\n",
      "Requirement already satisfied: starlette>=0.27 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from mcp>=1.10.0->fastmcp) (0.47.1)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from mcp>=1.10.0->fastmcp) (0.35.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (0.4.1)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from pydantic[email]>=2.11.7->fastmcp) (2.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp) (2.19.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from typer>=0.15.2->fastmcp) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from typer>=0.15.2->fastmcp) (1.5.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from anyio->httpx>=0.28.1->fastmcp) (1.3.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp) (2.7.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (0.26.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (0.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp) (0.1.2)\n",
      "Requirement already satisfied: cffi>=1.14 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from cryptography->authlib>=1.5.2->fastmcp) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from cffi>=1.14->cryptography->authlib>=1.5.2->fastmcp) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/workshop-private-agentic-ai/.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<0.4,>=0.3.36->langchain_mcp_adapters) (2.5.0)\n",
      "Downloading langgraph-0.5.1-py3-none-any.whl (143 kB)\n",
      "Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
      "Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
      "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
      "Installing collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "Successfully installed langgraph-0.5.1 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.72 ormsgpack-1.10.0 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastmcp langchain_mcp_adapters langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772e84e",
   "metadata": {},
   "source": [
    "## **MCP server**\n",
    "\n",
    "Voor de mensen die REST API's met HTTP endpoints kennen: MCP is hier een equivalent van maar dan met \"tools\". <br>\n",
    "\n",
    "**Tools zijn acties** die iets **berekenen of uitvoeren** in opdracht van de LLM. <br>\n",
    "We hebben dus een computer of **MCP server** nodig die luistert naar binnenkomende instructies en deze uitvoert. <br>\n",
    "Welke tools er allemaal beschikbaar zijn en wat ze precies doen, wordt allemaal gedefinieerd op deze server. <br>\n",
    "\n",
    "Er bestaan al veel **publieke MCP servers** die je kan gebruiken, bv. om het weerbericht op te vragen. <br>\n",
    "Om de focus van deze workshop bij **privacy** te houden, gaan wij zelf √©√©n maken en hosten. <br>\n",
    "\n",
    "Neem een kijkje naar de code van `mcp-server.py`. Probeer te beschrijven wat elke functie doet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65be92",
   "metadata": {},
   "source": [
    "**Start de MCP server** door het commando uit te voeren in de terminal:\n",
    "\n",
    "```bash\n",
    "python \"3 - MCP/mcp-server.py\"\n",
    "```\n",
    "<img src=\"../.github/fastmcp.png\" alt=\"FastMCP\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784387b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31458936",
   "metadata": {},
   "source": [
    "## **MCP client**\n",
    "\n",
    "Nu de server met onze tools draait, kunnen **MCP clients** ermee connecteren. <br>\n",
    "Een client kan ook perfect met meerdere servers connecteren, dan heeft deze toegang tot de tools van alle servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed1761",
   "metadata": {},
   "source": [
    "### **1. Python script als client**\n",
    "Een MCP client is typisch een taalmodel, maar we kunnen ook d.m.v. een Python script **manueel ermee verbinden** en tools aanspreken. <br>\n",
    "Dit kan handig zijn om te **testen** of de server en zijn tools goed werken, alvorens we er een LLM op loslaten. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31949497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastmcp import Client\n",
    "\n",
    "config = {\n",
    "    \"mcpServers\": {\n",
    "        \"private\": {\"url\": \"http://localhost:8000/sse\"},    # Onze eigen lokale MCP server\n",
    "    },\n",
    "}\n",
    "\n",
    "client = Client(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3949027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tools:\n",
      "  - sum: Use this tool to calculate the sum of two numbers.\n",
      "  - multiply: Use this tool to calculate the product of two numbers.\n",
      "  - datetime: Use this tool to get the current date and time.\n",
      "  - search_files: Use this tool to search for relevant information in the user's files and documents. Provide a search query or keywords to this tool.\n",
      "\n",
      "Resources:\n",
      "\n",
      "Prompts:\n",
      "[./Blogpost.pdf - pagina 2]\n",
      "Voorbeelden: op Ollama kan je van sommige taalmodellen - zoals de gemma3 \n",
      "familie - ook QAT-varianten terugvinden die tot 3 keer sneller zijn \n",
      "(https://ollama.com/library/gemma3ÓÇí4b-it-qat). Ook van computervisie \n",
      "modellen zoals YOLO bestaan PTQ-varianten zoals YOLOv8ÓÇàDetection-\n",
      "Quantized (https://huggingface.co/qualcomm/YOLOv8ÓÇàDetection-Quantized).\n",
      "Frameworks\n",
      "Er bestaan verschillende Python-libraries die functies bevatten om jouw model \n",
      "te quantiseren:\n",
      "\n",
      "[./Blogpost.pdf - pagina 4]\n",
      "zoals DeepSeek-R1ÓÇàDistill-Qwen-7B (https://huggingface.co/deepseek-\n",
      "ai/DeepSeek-R1ÓÇàDistill-Qwen-7BÓÇÇ. Een ouder succesverhaal van distillatie is \n",
      "TinyBERT. Die is ruim 7 keer kleiner en 9 keer sneller dan het oorspronkelijke \n",
      "BERT terwijl het slecht een daling van ¬±3% heeft in nauwkeurigheid \n",
      "(https://arxiv.org/pdf/1909.10351ÓÇÇ.\n",
      "Samenvatting\n",
      "Blogpost\n",
      "4\n",
      "\n",
      "[./Blogpost.pdf - pagina 4]\n",
      "Ook hier kan je het model hertrainen of finetunen na het prunen om het kleine \n",
      "verlies in accuraatheid te compenseren.\n",
      "Frameworks\n",
      "Er bestaan Python-libraries die functies bevatten om jouw model te prunen:\n",
      "Gebruik de ‚Äútorch.nn.utils.pruneÀÆ library voor PyTorch modellen.\n",
      "Gebruik de ‚Äútensorflow_model_optimization.sparsity.kerasÀÆ library voor \n",
      "TensorFlow modellen.\n",
      "Ook Hugging Face ºs Optimum library beschikt over pruning mogelijkheden \n",
      "voor Intel-systemen.\n",
      "3. Kennisdistillatie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async with client:\n",
    "    \n",
    "    print(\"\\nTools:\")\n",
    "    tools = await client.list_tools()\n",
    "    for tool in tools:\n",
    "        print(f\"  - {tool.name}: {tool.description}\")\n",
    "        \n",
    "    print(\"\\nResources:\")\n",
    "    resources = await client.list_resources()\n",
    "    for resource in resources:\n",
    "        print(f\"  - {resource.name}: {resource.description}\")\n",
    "\n",
    "    print(\"\\nPrompts:\")\n",
    "    prompts = await client.list_prompts()\n",
    "    for prompt in prompts:\n",
    "        print(f\"  - {prompt.name}: {prompt.description}\")\n",
    "\n",
    "    a = 5\n",
    "    b = 3\n",
    "    result = await client.call_tool(\"search_files\", {\"question\": f\"What can you tell me about blogpost.pdf?\"})\n",
    "    print(result.content[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069c387",
   "metadata": {},
   "source": [
    "### **2. Taalmodel als client**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a51453",
   "metadata": {},
   "source": [
    "Nu gaan we een taalmodel toegang geven tot de tools op onze MCP server. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004cb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "client = MultiServerMCPClient({\"private\": {\"url\": \"http://localhost:8000/sse\", \"transport\": \"sse\"}})\n",
    "tools = await client.get_tools()\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "agent = create_react_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899e5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üôç‚Äç‚ôÇÔ∏è User:\tWhat is the sum of 5 and 3?\n",
      "ü§ñ Calling tool(s) sum\n",
      "ü§ñ Milvus:\tThe sum of 5 and 3 is 8.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "\n",
    "history = []\n",
    "history.append({\"role\": \"system\", \"content\": \"Your name is Milvus, you are a helpful assistant that answers questions. You have access to some tools to assist you. If you fetch context from the user's files, always include the source in your answer.\"})\n",
    "\n",
    "while True:\n",
    "    question = input(\"Type your next question (/bye to stop): \")\n",
    "    if question.strip().lower() == \"/bye\":\n",
    "        print(\"ü§ñ Milvus:\\tGoodbye!\")\n",
    "        break\n",
    "    if not question.strip():\n",
    "        continue\n",
    "    \n",
    "    print(f\"üôç‚Äç‚ôÇÔ∏è User:\\t{question}\")\n",
    "    history.append({\"role\": \"user\", \"content\": question})\n",
    "    outputs = await agent.ainvoke({\"messages\": history})\n",
    "    \n",
    "    for output in outputs[\"messages\"]:\n",
    "        if isinstance(output, AIMessage):\n",
    "            if output.tool_calls:\n",
    "                print(f\"ü§ñ Calling tool(s) {', '.join(tc['name'] for tc in output.tool_calls)}\")\n",
    "        #elif isinstance(output, ToolMessage):\n",
    "            # print(f\"üõ†Ô∏è Tool:\\t{output.content}\")\n",
    "            \n",
    "    answer = outputs[\"messages\"][-1].content\n",
    "    print(f\"ü§ñ Milvus:\\t{answer}\")\n",
    "    history.append({\"role\": \"assistant\", \"content\": answer})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
